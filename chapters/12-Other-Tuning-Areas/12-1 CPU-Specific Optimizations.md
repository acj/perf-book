## CPU-Specific Optimizations {.unlisted .unnumbered}

Optimizing software for a specific CPU microarchitecture involves tailoring your code to leverage the strengths and mitigate the weaknesses of that microarchitecture. It is easier to do when you know the exact target CPU for your application. However, most application run on a wide range of CPUs. Optimizing performance of a cross-platform application with a very high speed requirements can be challenging since platforms from different vendors have different designs and implementations. Nevertheless, it is possible to write code that performs reasonably well on CPUs from different vendors, while providing a fine-tuned version for a specific microarchitecture. 

The major differences between x86 (considered as CISC) and RISC ISAs, such as ARM and RISC-V, are summarized below:

* x86 instructions are variable-length, while ARM and RISC-V instructions are fixed-length. This makes decoding x86 instructions more complex.
* x86 ISA has many addressing modes, while ARM and RISC-V have few addressing modes. Operands in ARM and RISC-V instructions are either registers or immediate values, while x86 instruction inputs can also come from memory. This bloats the number of x86 instructions, but also allows for more powerful single instructions. For instance, ARM requires to load a memory location first, then perform the operation; x86 can do both in one instruction.

In addition to this, there are a few other differences which you should consider when optimizing for a specific microarchitecture. As of 2024, the most recent x86-64 ISA has 16 architectural general-purpose registers, while the latest ARMv8 and RV64 require a CPU to provide 32 general-purpose registers. Although Intel has announced a new extension called APX[^1] that will increase the number of registers to 32. ARM and RISC-V do not have a dedicated `FLAGS` register, which eliminates unnecessary dependency chains on the `FLAGS` register. Finally, there is a difference in the memory page size between x86 and ARM. The default page size for x86 platforms is 4 KB, while most ARM systems (for example, macOS) use a 16 KB page size, although both platforms support larger page sizes (see [@sec:ArchHugePages], and [@sec:secDTLB]). All these differences can affect the performance of your application when it becomes a bottleneck.

Although ISA differences *may* have a tangible impact on performance of a specific application, numerous studies show that on average, differences between the two most popular ISAs, namely x86 and ARM, don't have a measurable performance impact. Throughout this book, we carefully avoided advertisements of any products (e.g., Intel vs. AMD vs. Apple) and any religious ISA debates (x86 vs. ARM vs. RISC-V). Below are some references that we hope will close the debate:

* Performance or energy consumption differences are not generated by ISA differences, but rather by microarchitecture implementation. [@RISCvsCISC2013]
* ISA doesn't have a large effect on the number and type of executed instructions. [@RISCVvsAArch642023] [@RISCvsCISC2013]
* CISC code is not denser than RISC code. [@CodeDensityCISCvsRISC]
* ISA overheads can be effectively mitigated by microarchitecture implementation. For example, $\mu$op cache minimizes decoding overheads; instruction cache minimizes code density impact. [@RISCvsCISC2013] [@ChipsAndCheesex86]

Nevertheless, this doesn't remove the value of architecture-specific optimizations. In this section, we will discuss how to optimize for a particular platform. We will cover ISA extensions, instruction latencies and throughput, and discuss some common microarchitecture-specific issues.

### ISA Extensions {.unlisted .unnumbered}

ISA evolution has been continuous, it has focused on accelerating specialized workloads, such as cryptography, AI, multimedia, and others. Utilizing ISA extensions often results in lucrative performance improvements. Developers keep finding smart ways to leverage these extensions in general-purpose applications. So, even if you're outside of one of these highly specialized domains, you might still benefit from using ISA extensions. That's why we suggest you to familiarize yourself with the ISA extensions available on your target platform.

It's not possible to learn about all specific instructions. But we suggest you to familiarize yourself with major ISA extensions and their capabilities. For example, if you are developing an AI application that uses `fp16` data types, and you target one of the modern ARM processors, make sure that your program's machine code contains corresponding `fp16` ISA extensions. If you're developing encyption/decryption software, check if it utilizes crypto extensions of your target ISA. And so on.

Here is a list of some notable x86 ISA extensions:

* SSE/AVX/AVX2: provide SIMD instructions for floating-point and integer operations.
* AVX512: extends AVX2 with 512-bit registers and many new instructions.
* AVX512_FP16/AVX512_BF16: adds support for 16-bit half-precision and `Bfloat16` floating-point values.
* AES/SHA: provide instructions for AES encryption, decryption, and SHA hashing.
* BMI/BMI2: provide instructions for bit manipulation.
* AVX_VNNI/AVX512_VNNI: Vector Neural Network Instructions for accelerating deep learning workloads.
* AMX: Advanced Matrix Extensions for accelerating matrix multiplication.

Here is a list of some notable ARM ISA extensions:

* asimd: also known as neon, provides SIMD instructions for floating-point and integer operations.
* aes/sha1/sha2/sha3/sha512/crc32: provide instructions for encryption, hashing, and checksumming.
* fp16/bf16: provides 16-bit half-precision floating-point instructions.
* dotprod: support for dot product instructions for accelerating machine learning workloads.
* sve: enables scalable vector length instructions.
* sme: Scalable Matrix Extension for accelerating matrix multiplication.

When compiling your applications, make sure to enable the necessary compiler flags to activate required ISA extensions. On GCC and Clang compilers use `-march` option. For example, `-march=native` will activate ISA features of your host system, i.e., on which you run the compilation. Or you can include specific version of ISA, e.g., `-march=armv8.6-a`. On MSVC compiler, use `/arch` option, e.g., `/arch:AVX2`.

#### CPU dispatch

These advice are mostly about compute-bound loops

Use compile-time or runtime checks to introduce platform-specific optimizations. This technique is called CPU dispatching. It allows you to write a single codebase that can be optimized for different microarchitectures. For example, you can write a generic implementation of a function and then provide microarchitecture-specific implementations that are used when the target CPU supports certain instructions. For example:

```cpp
if (__builtin_cpu_supports ("avx512f")) {
  avx512_impl();
} else {
  generic_impl();
}
```

https://johnnysswlab.com/cpu-dispatching-make-your-code-both-portable-and-fast/

As a rule of thumb, it is better to start with a generic implementation and then introduce microarchitecture-specific optimizations progressively, ensuring there is a fallback for architectures that do not have required features.

### Instruction latencies and throughput {.unlisted .unnumbered}

Execution Units: Identify the types and numbers of execution units (e.g., ALUs, FPUs).
[MOVE] Cache Hierarchy: Understand the levels of cache, their sizes, and their latencies.

These aspects are often publicly accessible in the CPU's datasheet or technical reference manual.

Other details of a microarchitecture might not be public, such as sizes of branch prediction history buffers, branch misprediction penalty, instructions latencies and throughput. While this information is not disclosed by CPU vendors, people have reverse-engineered some of it, which can be found online.

How to reason about instruction latencies and throughput?

Be very careful about making conclusions just on the numbers. In many cases, instruction latencies are hidden by the out-of-order execution engine, and it may not matter if an instruction has latency of 4 or 8 cycles. If it doesn't block forward progress, this instruction will be handled "in the background" without harming performance. However, latency of an instuction becomes important when it stands on a critical dependency chain of instructions because it delays execution of dependant operations.

In contrast, if you have a loop that performs a lot of independent operations, you should focus on instruction throughput rather than latency. When operations are independent, they can be processed in parallel. In such scenario, the critical factor is how many operations of a certain type can be executed per cycle, or *execution throughput*. Even if an instruction has a high latency, the out-of-order execution engine can hide it. Keep in mind, there are also "in between" scenarios, where both instruction latency and throughput may affect performance.

Port contention: you'll find a lot of stuff *has* to go to p5. So one of the challenges is to find ways of substituting things that aren't p5. If you're heavily bottlenecked enough of p5, then you may find that 2 ops on p0 are better than 1 op on p5.

### Microarchitecture-specific issues {.unlisted .unnumbered}

#### Memory ordering {.unlisted .unnumbered}
example with histogram
Once memory operations are in their respective queues, the load/store unit has to make sure memory ordering is preserved.
When load is executing it has to be checked against all older stores for potential store forwarding. But some stores might still have their address unknown. The LSU has to apply memory disambiguation prediction to decide if load can proceed ahead of unknown stores or not. And clearly load cannot forward from a store which address is still unknown.
#### when FMA contraction hurts performance
example with nanobench
#### Memory alignment {.unlisted .unnumbered}
example with split loads in matmul
#### 4K aliasing {.unlisted .unnumbered}
just describe
https://richardstartin.github.io/posts/4k-aliasing
#### Cache trashing {.unlisted .unnumbered}
just describe
Avoid Cache Thrashing: Minimize cache conflicts by ensuring data structures do not excessively map to the same cache lines.
https://github.com/ibogosavljevic/hardware-efficiency/blob/main/cache-conflicts/cache_conflicts.c
#### AVX-SSE Transitions {.unlisted .unnumbered}
just describe
#### Non-temporal stores {.unlisted .unnumbered}
remove?

[^1]: Intel APX - [https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html](https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html)