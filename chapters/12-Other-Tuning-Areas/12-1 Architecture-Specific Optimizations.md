## Architecture-Specific Optimizations {.unlisted .unnumbered}

[TODO]: improve intro

Optimizing software for a specific CPU microarchitecture involves tailoring your code and compilation process to leverage the strengths and mitigate the weaknesses of that microarchitecture. Here's a step-by-step guide to using knowledge of a target CPU microarchitecture to optimize your software:

Progressive Enhancement:
Default to Generic: Write code to perform well on a baseline architecture.
Add Optimizations: Introduce specific optimizations progressively, ensuring there is a fallback for architectures that do not support them.

When developing cross-platform applications where the exact target CPU configuration is unknown, you can still apply microarchitecture-specific optimizations in a general and adaptable way.

Developing a cross-platform application with a very high performance requirements can be challenging since platforms from different vendors have different implementations. The major difference between x86 (considered as CISC) and RISC ISAs, such as ARM and RISC-V, are summarized below:

* x86 instructions are variable-length, while ARM and RISC-V instructions are fixed-length. This makes decoding x86 instructions more complex.
* x86 ISA has many addressing modes, while ARM and RISC-V have few addressing modes. Operands in ARM and RISC-V instructions are either registers or immediate values, while x86 instruction inputs can also come from memory. This bloats the number of x86 instructions, but also allows for more powerful single instructions. For instance, ARM requires to load a memory location first, then perform the operation; x86 can do both in one instruction.

In addition to this, there are a few other differences which you should consider when optimizing for a specific microarchitecture. As of 2024, the most recent x86-64 ISA has 16 architectural general-purpose registers, while the latest ARMv8 and RV64 require a CPU to provide 32 general-purpose registers. Although Intel has announced a new extension called APX[^1] that will increase the number of registers to 32. ARM and RISC-V do not have a dedicated `FLAGS` register, which eliminates unnecessary dependency chains on the `FLAGS` register. Finally, there is a difference in the memory page size between x86 and ARM. The default page size for x86 platforms is 4 KB, while most ARM systems (for example, macOS) use a 16 KB page size, although both platforms support larger page sizes (see [@sec:ArchHugePages], and [@sec:secDTLB]). All these differences can affect the performance of your application when it becomes a bottleneck.

Although ISA differences *may* have a tangible impact on performance of a specific application, numerous studies show that on average, differences between the two most popular ISAs, namely x86 and ARM, don't have a measurable performance impact. Throughout this book, we carefully avoided advertisements of any products (e.g., Intel vs. AMD vs. Apple) and any religious ISA debates (x86 vs. ARM vs. RISC-V). Below are some references that we hope will close the debate:

* Performance or energy consumption differences are not generated by ISA differences, but rather by microarchitecture implementation. [@RISCvsCISC2013]
* ISA doesn't have a large effect on the number and type of executed instructions. [@RISCVvsAArch642023] [@RISCvsCISC2013]
* CISC code is not denser than RISC code. [@CodeDensityCISCvsRISC]
* ISA overheads can be effectively mitigated by microarchitecture implementation. For example, $\mu$op cache minimizes decoding overheads; instruction cache minimizes code density impact. [@RISCvsCISC2013] [@ChipsAndCheesex86]

Nevertheless, this doesn't remove the value of architecture-specific optimizations. 
[TODO] Outline what we will be talking about: ISA Extensions, Instruction latencies and throughput, Microarchitecture-specific issues.

### ISA Extensions {.unlisted .unnumbered}

[TODO]: list the most important ISA extensions

ISA evolution has been continuous, it has focused on enabling specialization

Instruction Set Extensions: Familiarize yourself with available SIMD, cryptographic, and other specialized instructions.

It's not possible to learn about all specific instructions. But we suggest you to familiarize yourself with major ISA extensions and their capabilities. For example, if you are developing an AI application that uses `fp16` data types, and you target one of the modern ARM processors, make sure that your program's machine code contains corresponding `fp16` ISA extensions. If you're developing encyption/decryption software, check if it utilizes crypto extensions of your target ISA.
Provide a list of these extensions?
(e.g. AES, VNNI, AVX512FP16 and AVX512BF16, AMX, ARM fp16, DOT, SVE, SME)
Use compiler flags that generate code optimized for your target CPU. 
GCC/Clang: -march=native or -march=[architecture] (e.g., -march=skylake)
Intel Compiler: -xHost or -x[architecture] (e.g., -xCORE-AVX512)
MSVC: /arch:[architecture] (e.g., /arch:AVX2)

These advice are mostly about compute-bound loops

#### CPU dispatch

Use compile-time or runtime checks to introduce platform-specific optimizations. This technique is called CPU dispatching. It allows you to write a single codebase that can be optimized for different microarchitectures. For example, you can write a generic implementation of a function and then provide microarchitecture-specific implementations that are used when the target CPU supports certain instructions. For example:

```cpp
if (__builtin_cpu_supports ("avx512f")) {
  avx512_impl();
} else {
  generic_impl();
}
```

https://johnnysswlab.com/cpu-dispatching-make-your-code-both-portable-and-fast/

### Instruction latencies and throughput {.unlisted .unnumbered}

Execution Units: Identify the types and numbers of execution units (e.g., ALUs, FPUs).
[MOVE] Cache Hierarchy: Understand the levels of cache, their sizes, and their latencies.

These aspects are often publicly accessible in the CPU's datasheet or technical reference manual.

Other details of a microarchitecture might not be public, such as sizes of branch prediction history buffers, branch misprediction penalty, instructions latencies and throughput. While this information is not disclosed by CPU vendors, people have reverse-engineered some of it, which can be found online.

How to reason about instruction latencies and throughput?

Be very careful about making conclusions just on the numbers. In many cases, instruction latencies are hidden by the out-of-order execution engine, and it may not matter if an instruction has latency of 4 or 8 cycles. If it doesn't block forward progress, this instruction will be handled "in the background" without harming performance. However, latency of an instuction becomes important when it stands on a critical dependency chain of instructions because it delays execution of dependant operations.

In contrast, if you have a loop that performs a lot of independent operations, you should focus on instruction throughput rather than latency. When operations are independent, they can be processed in parallel. In such scenario, the critical factor is how many operations of a certain type can be executed per cycle, or *execution throughput*. Even if an instruction has a high latency, the out-of-order execution engine can hide it. Keep in mind, there are also "in between" scenarios, where both instruction latency and throughput may affect performance.

Port contention: you'll find a lot of stuff *has* to go to p5. So one of the challenges is to find ways of substituting things that aren't p5. If you're heavily bottlenecked enough of p5, then you may find that 2 ops on p0 are better than 1 op on p5.

### Microarchitecture-specific issues {.unlisted .unnumbered}

#### Memory ordering {.unlisted .unnumbered}
example with histogram
Once memory operations are in their respective queues, the load/store unit has to make sure memory ordering is preserved.
When load is executing it has to be checked against all older stores for potential store forwarding. But some stores might still have their address unknown. The LSU has to apply memory disambiguation prediction to decide if load can proceed ahead of unknown stores or not. And clearly load cannot forward from a store which address is still unknown.
#### when FMA contraction hurts performance
example with nanobench
#### Memory alignment {.unlisted .unnumbered}
example with split loads in matmul
#### 4K aliasing {.unlisted .unnumbered}
just describe
https://richardstartin.github.io/posts/4k-aliasing
#### Cache trashing {.unlisted .unnumbered}
just describe
Avoid Cache Thrashing: Minimize cache conflicts by ensuring data structures do not excessively map to the same cache lines.
https://github.com/ibogosavljevic/hardware-efficiency/blob/main/cache-conflicts/cache_conflicts.c
#### AVX-SSE Transitions {.unlisted .unnumbered}
just describe
#### Non-temporal stores {.unlisted .unnumbered}
remove?

[^1]: Intel APX - [https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html](https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html)