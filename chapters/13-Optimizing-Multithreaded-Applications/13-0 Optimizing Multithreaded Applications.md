# Optimizing Multithreaded Applications {#sec:secOptMTApps}

Modern CPUs are getting more and more cores each year. As of 2024, you can buy a server processor which will have more than 200 cores! And even a mid-range laptop with 16 execution threads is a pretty usual setup nowadays. Since there is so much processing power in every CPU, effective utilization of all the HW threads becomes more challenging. Preparing software to scale well with a growing amount of CPU cores is very important for the future success of the application.

There is a difference in how server and client products exploit parallelism. Most server platforms are designed to process requests from a large number of customers. Those requests are usually independent of each other, so the server can process them in parallel. If there is enough load on the system, applications themselves could even be single-threaded and still platform utilization will be high. Obviously, the situation changes drastically, if you start using your server for HPC or AI computations; then you need all the computing power there is. On the other hand, client platforms, such as laptops and desktops, have all the resources to serve a single user. In this case, an application has to make use of all the available cores to provide the best user experience.

Multithreaded applications have their own specifics. Certain assumptions of single-threaded execution become invalid when we start dealing with multiple threads. For example, we can no longer identify hotspots by looking at a single thread since each thread might have its own hotspot. In a popular [producer-consumer](https://en.wikipedia.org/wiki/Producerâ€“consumer_problem)[^5] design, the producer thread may sleep during most of the time. Profiling such a thread won't shed light on the reason why a multithreaded application is performing poorly.

There are two main ways to to achieve software parallelism: multiprocessing and multithreading. In a multiprocessed application, multiple independent processes run concurrently. Each process has its own memory space and communicate with other processes through inter-process communication (IPC) mechanisms such as pipes, sockets, or shared memory. In a multithreaded application, single process contains multiple threads, which share the same memory space and resources of the process. Threads within the same process can communicate and share data more easily because they have direct access to the same memory space. However, synchronization between threads is usually more complex and is prone to issues like race conditions and deadlocks. In this chapter we will mostly focus on multithreaded applications, however some techniques can be applied to multiprocessed applications as well. In fact, we will show examples of both types of applications in this chapter.

[TODO]: can I reference some broad study on the scaling of workloads with the number of cores?

Some production applications scale well with the number of cores. However, there are many applications that don't scale at all. On the two extremes we have the following two types of multithreaded applications:

* **Massively parallel**. This type of applications usually scales well with the number of cores. They are designed to process a large number of independent tasks. Massively parallel programms often use divide-and-conquer technique to split the work into smaller tasks (also called *worker threads*) and process them in parallel. Examples of such applications are scientific computations, video rendering, data analytics, AI, and many others. The main challenge in this type of applications is that saturating a shared resource, such as memory bandwidth, can effectively stall all the worker threads in the process.
* **Require synchronization**.

[TODO]: Preview what we will talk about: first explore massively parallel, then explore how to find expensive locks.

[^5]: Producer-consumer pattern - [https://en.wikipedia.org/wiki/Producer-consumer_problem](https://en.wikipedia.org/wiki/Producer-consumer_problem)
